{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef01df6-c63e-4393-a556-45679d3ad90d",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n",
    "\n",
    "[Machine Learning Mastery](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Attention_(machine_learning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc65d04-8395-44a5-a3cf-af24758b7916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98522025 1.74174051 0.75652026]\n"
     ]
    }
   ],
   "source": [
    "# 1. Encode sequence of words (with encoder, or manually)\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# encoder representations of four different words\n",
    "word_1 = np.array([1, 0, 0])\n",
    "word_2 = np.array([0, 1, 0])\n",
    "word_3 = np.array([1, 1, 0])\n",
    "word_4 = np.array([0, 0, 1])\n",
    "\n",
    "# 2. Init Query, Key, Value weights\n",
    "# generating the weight matrices\n",
    "np.random.seed(42) # to allow us to reproduce the same attention values\n",
    "W_Q = np.random.randint(3, size=(3, 3))  # Dim = Embedding dim x Attention dim\n",
    "W_K = np.random.randint(3, size=(3, 3))\n",
    "W_V = np.random.randint(3, size=(3, 3))\n",
    "\n",
    "# 3. Compute Q, K, V vectors\n",
    "# generating the queries, keys and values\n",
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    "\n",
    "query_2 = word_2 @ W_Q\n",
    "key_2 = word_2 @ W_K\n",
    "value_2 = word_2 @ W_V\n",
    "\n",
    "query_3 = word_3 @ W_Q\n",
    "key_3 = word_3 @ W_K\n",
    "value_3 = word_3 @ W_V\n",
    "\n",
    "query_4 = word_4 @ W_Q\n",
    "key_4 = word_4 @ W_K\n",
    "value_4 = word_4 @ W_V\n",
    "\n",
    "# 4. Get dot product similarity between Query 1 and Keys 1-4\n",
    "#    Q1.T @ Ki for i=1:4 -> Weights\n",
    "# scoring the first query vector against all key vectors\n",
    "scores = np.array([\n",
    "    np.dot(query_1, key_1), \n",
    "    np.dot(query_1, key_2), \n",
    "    np.dot(query_1, key_3), \n",
    "    np.dot(query_1, key_4)\n",
    "])\n",
    "\n",
    "# computing the weights by a softmax operation\n",
    "weights = softmax(scores / key_1.shape[0] ** 0.5)\n",
    "\n",
    "# 5. Attention = SUM ( Weights.T @ Values )\n",
    "# computing the attention by a weighted sum of the value vectors\n",
    "attention = (weights[0] * value_1) + (weights[1] * value_2) + (weights[2] * value_3) + (weights[3] * value_4)\n",
    "\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb497ccc-1e0c-4ded-815f-d06232d6a769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6a33c-f9b9-4013-b279-32b156e88646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f8316-c91a-4fc4-80b4-bd4933e3a013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
